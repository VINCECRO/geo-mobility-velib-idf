# VÃ©lib' Geo-Mobility Analytics Platform â€” Ãle-de-France

> **End-to-end data engineering pipeline** â€” real-time ingestion Â· SCD Type 2 Â· PostGIS spatial joins Â· dbt layered modeling Â· geospatial ML

<br>

## Overview

A **production-grade data platform** for VÃ©lib' bike-sharing analytics across the Ãle-de-France region. The project covers the full data lifecycle: real-time API ingestion every 5 minutes, historized station metadata with SCD Type 2, a fully layered dbt transformation stack (staging â†’ intermediate â†’ marts), geospatial enrichment with PostGIS spatial joins and high-resolution population data, ML-based territory clustering, and BI visualization with Apache Superset.

Designed as a **technical portfolio demonstrator**, it applies patterns typically found in professional data teams â€” distributed Airflow orchestration, hash-based change detection, SCD2-aware spatial aggregation, incremental materialization with `delete+insert` strategy, and custom dbt tests for data quality enforcement.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VÃ©lib' Open API                          â”‚
â”‚       (IDFM Marketplace â€” station_information + status)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚  every 5 min
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Apache Airflow 3.x                         â”‚
â”‚         CeleryExecutor Â· Redis broker Â· Python 3.12         â”‚
â”‚                                                             â”‚
â”‚  velib_extract_ingestion_dag  (*/5 * * * *)                 â”‚
â”‚    extract_station â”€â”€â–º load_stations  (SCD2 upsert)         â”‚
â”‚    extract_status  â”€â”€â–º load_status    (append-only)         â”‚
â”‚                                                             â”‚
â”‚  dbt_dag  (:03 and :33 past each hour)                      â”‚
â”‚    ExternalTaskSensor â”€â”€â–º dbt run â”€â”€â–º dbt test              â”‚
â”‚                       â”€â”€â–º dbt docs generate                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PostgreSQL 15 + PostGIS 3.4                       â”‚
â”‚                                                             â”‚
â”‚  Schema: raw                                                â”‚
â”‚    stations_scd      â† SCD2 with MD5 hash diff             â”‚
â”‚    station_status    â† append-only time series (5 min)      â”‚
â”‚                                                             â”‚
â”‚  Schema: add_assets  (loaded at container startup)          â”‚
â”‚    communes_idf      â† IDF commune boundaries (EPSG:4326)   â”‚
â”‚    pop_commune_idf   â† commune-level population             â”‚
â”‚    pop_pointwise_idf â† Meta HRPD 30m grid (EPSG:2154)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        dbt                                   â”‚
â”‚                                                             â”‚
â”‚  staging/                                                   â”‚
â”‚    stg_velib_station_current     â† SCD2 filter current only â”‚
â”‚    stg_velib_station_historical  â† full SCD2 history        â”‚
â”‚    stg_velib_station_status      â† incremental (del+insert) â”‚
â”‚    stg_geo_communes_idf          â† CRS transform â†’ 4326     â”‚
â”‚    stg_geo_pop_communes          â† commune population       â”‚
â”‚    stg_geo_pop_idf               â† 30m grid + GiST index    â”‚
â”‚                                                             â”‚
â”‚  intermediate/                                              â”‚
â”‚    int_station_current_geo_enriched   â† ST_Within commune   â”‚
â”‚    int_station_historical_geo_enrichedâ† SCD2-aware commune  â”‚
â”‚    int_station_pop_500m               â† ST_DWithin 500m pop â”‚
â”‚    int_station_status_with_capacity   â† SCD2 capacity join  â”‚
â”‚    int_station_status_within_500m     â† real-time neighbor  â”‚
â”‚    int_station_status_hourly          â† hourly aggregation  â”‚
â”‚                                                             â”‚
â”‚  marts/core/                                                â”‚
â”‚    dim_station           â† enriched SCD2 dimension          â”‚
â”‚    fct_station_availability â† fact table w/ 500m context   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Visualization Layer                             â”‚
â”‚   Apache Superset â€” geospatial dashboards, Redis cache      â”‚
â”‚   R/Shiny        â€” custom analytics platform (planned)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technical Highlights

### SCD Type 2 with hash-based change detection

Station metadata (location, capacity, name) is tracked with a full **Slowly Changing Dimension Type 2** strategy, implemented from scratch without any external SCD framework.

Each payload is fingerprinted with an MD5 hash over `(station_id, station_code, name, capacity, lat, lon)`. On each 5-minute cycle, the pipeline:
- **inserts** a new row if the station is new,
- **closes** the active version (`valid_to`, `current_validity = FALSE`) and **opens** a new one when the hash changes,
- **updates only `last_extracted_at`** if the hash is unchanged (heartbeat pattern, preserving lineage without row bloat).

The database enforces this with a **partial unique index** on `(station_id) WHERE current_validity = TRUE`, guaranteeing exactly one active version per station at all times. Every station relocation, capacity change, or rename is preserved with full temporal validity.

### SCD2-aware spatial joins throughout the transformation layer

The dbt intermediate layer is carefully designed to respect the SCD2 grain at every step. When joining station status snapshots to capacity, geometry, or population data, joins always apply the temporal predicate:

```sql
ON  sr.station_id   = scd.station_id
AND sr.extracted_at >= scd.valid_from
AND (scd.valid_to IS NULL OR sr.extracted_at < scd.valid_to)
```

This ensures that if a station was physically relocated, historical status snapshots are joined to the geometry that was valid *at that point in time* â€” not the current one.

### Real-time neighbor availability at each snapshot (`int_station_status_within_500m`)

One of the most technically involved models: for **each status snapshot**, it computes the total bikes and docks available within 500m of the station across all neighbor stations captured at that **same exact timestamp**.

This requires a three-way join: station status Ã— station geometry (SCD2-resolved) Ã— neighbor station geometry (also SCD2-resolved), filtered by `ST_DWithin(geometry_lambert, neighbor_geometry_lambert, 500)`. The model is materialized **incrementally** with a composite unique index on `(station_id, extracted_at)` and three additional indexes. All coordinates are projected to Lambert-93 (EPSG:2154) to use metric distances accurately.

This feeds directly into `fct_station_availability` where each row exposes both the station's own availability and its neighborhood context â€” enabling queries like *"was this station critically empty despite abundant supply nearby?"*

### High-resolution population aggregation at 500m (`int_station_pop_500m`)

For each station version (SCD2 grain: `station_id + valid_from`), the model sums all population points from Meta's High Resolution Population Density dataset (30m grid, CSTB 2019) within a 500m buffer. The geometry for each station version is resolved from its SCD2 record, so a relocated station gets a recomputed population score. Incremental materialization ensures only new SCD2 versions trigger a recalculation.

### `dim_station` â€” a fully enriched SCD2 dimension

The station dimension preserves the full history of station versions while enriching each with:
- **Commune** name, INSEE code, and population via `ST_Within` join,
- **Local population** within 500m from the high-resolution point grid,
- **NTILE-based capacity quartile** (`Q1-Small` to `Q4-XLarge`), computed on current stations and applied consistently to all versions,
- **Population density category** (`Peripheral` / `Suburban` / `Urban` / `Urban Core`), derived from current thresholds via `CROSS JOIN` with quartile CTEs,
- **`local_population_per_bike`** ratio: inhabitants within 500m per dock, a proxy for structural demand pressure.

### `fct_station_availability` â€” the central analytical fact table

Each row is one status snapshot, enriched with the full station context (capacity, commune, population) from `dim_station` plus 500m neighbor aggregates from `int_station_status_within_500m`. The model computes at grain level:
- `availability_rate` and `dock_availability_rate` (% of capacity),
- `is_bike_critical` / `is_dock_critical` flags (< 10% of capacity),
- `total_bikes_accessible_500m` / `total_docks_accessible_500m` (station + neighbors),
- temporal enrichment: `day_type` (Weekday/Weekend), `time_period` (Morning Rush, Evening Rush, Night, Off-Peak).

Materialized incrementally, partitioned by day on `extracted_at`, with four indexes including a composite unique on `(station_id, extracted_at)`.

### Hourly aggregation with operational KPIs (`int_station_status_hourly`)

Aggregates 5-minute snapshots into hourly slots via `DATE_TRUNC`. Per slot, it computes min/avg/max for bikes, docks, mechanical, and e-bikes, plus:
- `avg_bike_availability_pct`,
- `critical_bike_time_pct` and `critical_dock_time_pct` â€” the **fraction of the hour** the station spent in critical state (< 10% capacity),
- `is_complete_hour` flag (â‰¥ 10 snapshots),
- temporal annotations: `hour_of_day`, `day_of_week`, `day_type`, `time_period`.

### Custom dbt tests for data quality

Beyond standard `not_null` / `unique` / `accepted_values`, the project includes:

**Generic tests (Jinja macros):**
- `columns_must_match(column_a, column_b, tolerance)` â€” asserts two columns are equal within a numeric tolerance, with a detailed diff message per failing row.
- `sum_must_match(column_a, column_b, column_c)` â€” asserts `a + b = c`, detecting API inconsistencies in bike type breakdowns.

**Singular data tests:**
- `assert_bike_breakdown_sum` â€” validates `mechanical_available + ebikes_available = num_bikes_available` on the last 24h. Configured `severity: error`, `store_failures: true`.
- `assert_capacity_and_numdock` â€” checks `num_bikes_available + num_docks_available = capacity` on the latest snapshot. `severity: warn`.
- `assert_orphan_station_id_bidirectionnal` â€” performs a **FULL OUTER JOIN** between the latest `stations_scd` and `station_status` extractions, detecting stations present in one table but absent from the other in both directions (`orphan_in_status` / `orphan_in_stations`).

Source freshness is monitored with `warn_after: 15 minutes` and `error_after: 30 minutes` on `station_status`, using `extracted_at` as the loaded field.

### Orchestration with timing-aware DAG dependency

The ingestion DAG runs every 5 minutes. The dbt DAG runs at `:03` and `:33`, offset to let ingestion complete first. A custom `execution_date_fn` resolves the **most recently completed ingestion cycle** dynamically â€” not a static delta â€” avoiding race conditions when the two DAGs don't align exactly. The `ExternalTaskSensor` runs in `reschedule` mode (non-blocking) with a 15s poke interval and 3-minute timeout.

### Database initialization and geo-asset loading

The PostGIS database is initialized via a shell script in `docker-entrypoint-initdb.d`, creating the `raw` schema, both tables, all indexes (including the partial unique index for SCD2), and setting the database timezone to `Europe/Paris`.

A dedicated **`loader` container** starts after PostGIS is healthy and auto-discovers all `.gpkg` and `.csv` files in the mounted volume, loading them into the `add_assets` schema via `geopandas.to_postgis()`. Adding new geo-reference datasets is a simple file-drop operation â€” no SQL migration required.

### Geospatial feature engineering for ML clustering

A standalone pipeline enriches a **50m hexagonal grid** of the Ãle-de-France petite couronne (depts. 75, 92, 93, 94) with building usage profiles from the BDNB 2025 national database and Meta HRPD population density. Building usages are one-hot encoded and weighted by built surface area (mÂ²), aggregated by 500m centroid buffer in batches of 1,000 grid cells. The result feeds a **KMeans clustering** with `StandardScaler`, producing territory typologies (residential, commercial, mixed-use, peripheral) exported as GeoPackage for PostGIS ingestion.

---

## dbt Data Model

```
sources (raw + add_assets schemas)
    â”‚
    â”œâ”€â”€ staging/
    â”‚   â”œâ”€â”€ stg_velib_station_current       (filter: current_validity = TRUE)
    â”‚   â”œâ”€â”€ stg_velib_station_historical    (full SCD2 history)
    â”‚   â”œâ”€â”€ stg_velib_station_status        (incremental, delete+insert)
    â”‚   â”œâ”€â”€ stg_geo_communes_idf            (ST_Transform â†’ EPSG:4326)
    â”‚   â”œâ”€â”€ stg_geo_pop_communes            (commune-level population)
    â”‚   â””â”€â”€ stg_geo_pop_idf                 (30m grid, GiST index, EPSG:2154)
    â”‚
    â”œâ”€â”€ intermediate/ (views)
    â”‚   â”œâ”€â”€ int_station_current_geo_enriched     (ST_Within commune)
    â”‚   â”œâ”€â”€ int_station_historical_geo_enriched  (SCD2 Ã— commune Ã— pop)
    â”‚   â”œâ”€â”€ int_station_pop_500m                 (ST_DWithin 500m, incremental)
    â”‚   â”œâ”€â”€ int_station_status_with_capacity     (SCD2-resolved capacity join)
    â”‚   â”œâ”€â”€ int_station_status_within_500m       (real-time neighbor, incremental)
    â”‚   â””â”€â”€ int_station_status_hourly            (hourly KPIs + critical flags)
    â”‚
    â””â”€â”€ marts/core/ (tables)
        â”œâ”€â”€ dim_station           (SCD2 + NTILE quartiles + pop density category)
        â””â”€â”€ fct_station_availability  (fact table, partitioned by day, incremental)
```

---

## Stack

| Layer | Technology |
|---|---|
| Orchestration | Apache Airflow 3.x Â· CeleryExecutor Â· Redis |
| Storage | PostgreSQL 15 Â· PostGIS 3.4 |
| Transformation | dbt 1.8 Â· dbt-utils 1.1 |
| Geospatial | PostGIS Â· GeoPandas Â· BDNB 2025 Â· Meta HRPD |
| ML | scikit-learn (KMeans Â· StandardScaler) |
| Visualization | Apache Superset (Redis cache Â· Gunicorn) Â· R/Shiny *(planned)* |
| Infrastructure | Docker Â· Docker Compose |
| Dev tooling | Python 3.12 Â· UV |

---

## Repository Structure

```
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile                            # Airflow + dbt-postgres image
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ velib_ingestion_dag.py            # Extract & load DAG (*/5 min)
â”‚   â”‚   â””â”€â”€ velib_dbt_transform.py            # dbt DAG + ExternalTaskSensor
â”‚   â”œâ”€â”€ extract_Velib_API/
â”‚   â”‚   â”œâ”€â”€ velib_client.py                   # HTTP client + Paris timezone
â”‚   â”‚   â””â”€â”€ velib_parser.py                   # API payload deserializer
â”‚   â””â”€â”€ ingest_Velib_API/
â”‚       â”œâ”€â”€ db_connect.py                     # psycopg2 connection factory
â”‚       â”œâ”€â”€ ingest_station_SCD2.py            # SCD2 upsert with hash diff
â”‚       â””â”€â”€ ingest_station_status.py          # Append-only status writer
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml                       # Schema layout + materialization
â”‚   â”œâ”€â”€ packages.yml                          # dbt-utils 1.1.1
â”‚   â”œâ”€â”€ profiles.yml                          # env_var-based connection
â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â””â”€â”€ generate_schema_name.sql          # Custom schema routing macro
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ sources/                          # 4 source definitions + column tests
â”‚   â”‚   â”œâ”€â”€ staging/                          # 6 staging models
â”‚   â”‚   â”œâ”€â”€ intermediate/                     # 6 intermediate models (views)
â”‚   â”‚   â””â”€â”€ marts/core/                       # dim_station + fct_station_availability
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ generic/                          # columns_must_match, sum_must_match
â”‚       â””â”€â”€ data/                             # assert_bike_breakdown_sum
â”‚                                             # assert_capacity_and_numdock
â”‚                                             # assert_orphan_station_id_bidirectionnal
â”‚
â”œâ”€â”€ postgis/
â”‚   â”œâ”€â”€ init_script/01_Postgis_init.sh        # Schema + tables + indexes
â”‚   â””â”€â”€ additional_assets/
â”‚       â”œâ”€â”€ Dockerfile                        # Geo-asset loader container
â”‚       â”œâ”€â”€ import_assets.py                  # Auto-discover & load .gpkg/.csv
â”‚       â”œâ”€â”€ communes_Idf.gpkg                 # IDF commune boundaries
â”‚       â””â”€â”€ pop_commune_Idf.gpkg              # Commune-level population
â”‚
â”œâ”€â”€ computing_geoassets/
â”‚   â”œâ”€â”€ reducting_pop_to_idf.py              # Clip population to IDF communes
â”‚   â”œâ”€â”€ generating_pop_data.py               # Population GeoDataFrame builder
â”‚   â””â”€â”€ Geospatial_clustering.py             # BDNB + population â†’ KMeans
â”‚   # âš ï¸ The derived population files (pop_pointwise_idf) are not committed
â”‚   #    due to file size constraints. The scripts above fully reproduce them
â”‚   #    from the original Meta HRPD source data.
â”‚
â”œâ”€â”€ superset/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ superset_config.py                   # Redis cache, geo flags, SQL Lab
â”‚
â”œâ”€â”€ docker-compose-Airflow.yml               # Full stack (Airflow + PostGIS + loader)
â”œâ”€â”€ docker-compose-Superset.yml              # Superset on shared idfm_network
â””â”€â”€ pyproject.toml                           # UV-managed dependencies
```

---

## Getting Started

### Prerequisites

- Docker & Docker Compose
- A VÃ©lib' API key from the [IDFM Marketplace](https://prim.iledefrance-mobilites.fr)

### Environment setup

```bash
cp .env.example .env
# Fill in: VELIB_API_KEY, POSTGRES_*, AIRFLOW_*, AIRFLOW_JWT_SECRET, SUPERSET_*
```

> **Note on Airflow JWT:** A static `AIRFLOW_JWT_SECRET` is required to prevent authentication failures when containers are recreated. See [apache/airflow#49646](https://github.com/apache/airflow/issues/49646).

### Launch

```bash
# Start the pipeline stack (Airflow + PostGIS + geo-asset loader)
docker compose -f docker-compose-Airflow.yml up -d

# Start the visualization layer
docker compose -f docker-compose-Superset.yml up -d
```

| Service | URL |
|---|---|
| Airflow UI | http://localhost:8080 |
| dbt docs | http://localhost:8001 |
| Superset | http://localhost:8088 |

### Geospatial ML assets (optional)

Download BDNB 2025 building data for depts. 75, 92, 93, 94 from [bdnb.io](https://bdnb.io/download/) and Meta HRPD population CSV from [HDX](https://data.humdata.org/dataset/france-high-resolution-population-density-maps-demographic-estimates), then:

```bash
uv run python computing_geoassets/reducting_pop_to_idf.py
uv run python computing_geoassets/Geospatial_clustering.py
```

---

## Project Status

| Component | Status |
|---|---|
| Airflow ingestion DAGs | âœ… Operational |
| SCD Type 2 station tracking | âœ… Operational |
| Station status time series | âœ… Operational |
| PostgreSQL + PostGIS setup | âœ… Operational |
| Geo-asset loader (Docker) | âœ… Operational |
| dbt staging models (Ã—6) | âœ… Operational |
| dbt intermediate models (Ã—6) | âœ… Operational |
| dbt mart models (dim + fct) | âœ… Operational |
| Custom dbt tests (generic + singular) | âœ… Operational |
| Source freshness monitoring | âœ… Operational |
| Geospatial ML clustering pipeline | âœ… Operational |
| Superset geospatial dashboards | ğŸ”„ In progress |
| R/Shiny custom dashboard platform | ğŸ“‹ Planned |
| CI / automated dbt test pipeline | ğŸ“‹ Planned |

---

## Analytical Use Cases

The platform is designed to answer questions such as:

- Which stations are structurally undersupplied relative to surrounding population density?
- At which times and locations does critical unavailability occur â€” and is supply available nearby within 500m?
- How has the VÃ©lib' network evolved over time (station additions, relocations, capacity changes)?
- What is the ratio of local inhabitants to available docks per station, and how does it cluster by territory type?
- Which communes show the highest correlation between rush-hour demand and supply shortage?

---

## Author

**Vincent Crozet** â€” Data Engineer / Analyst Â· GIS Expert  
ğŸ“ Cotonou, Benin  
ğŸ”— [linkedin.com/in/vincent-crozet](https://www.linkedin.com/in/vincent-crozet)

---

*This repository is a portfolio project demonstrating realistic data engineering patterns, geospatial modeling, and analytics-driven pipeline design. It is not intended for direct production deployment.*